{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af5d969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image,caption\n",
      "1000092795.jpg, Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
      "1000092795.jpg,\" Two young , White males are outside near many bushes .\"\n",
      "1000092795.jpg, Two men in green shirts are standing in a yard .\n",
      "1000092795.jpg, A man in a blue shirt standing in a garden .\n"
     ]
    }
   ],
   "source": [
    "# Flickr30 path=\"/mnt/media/eleonora/flickr30k\"\n",
    "# Contains Images folder and captions.txt file and flickr_annotations_30k.csv file\n",
    "\n",
    "# For now let's open the text file and check the first few lines to understand the format of the captions\n",
    "with open(\"/mnt/media/eleonora/flickr30k/captions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[:5]:  # Print the first 5 lines\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec15ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw', 'sentids', 'split', 'filename', 'img_id']\n",
      "['[\"Two young guys with shaggy hair look at their hands while hanging out in the yard.\", \"Two young, White males are outside near many bushes.\", \"Two men in green shirts are standing in a yard.\", \"A man in a blue shirt standing in a garden.\", \"Two friends enjoy time spent together.\"]', '[0, 1, 2, 3, 4]', 'train', '1000092795.jpg', '0']\n",
      "['[\"Several men in hard hats are operating a giant pulley system.\", \"Workers look down from up above on a piece of equipment.\", \"Two men working on a machine wearing hard hats.\", \"Four men on top of a tall structure.\", \"Three men on a large rig.\"]', '[5, 6, 7, 8, 9]', 'train', '10002456.jpg', '1']\n",
      "['[\"A child in a pink dress is climbing up a set of stairs in an entry way.\", \"A little girl in a pink dress going into a wooden cabin.\", \"A little girl climbing the stairs to her playhouse.\", \"A little girl climbing into a wooden playhouse.\", \"A girl going into a wooden building.\"]', '[10, 11, 12, 13, 14]', 'train', '1000268201.jpg', '2']\n",
      "['[\"Someone in a blue shirt and hat is standing on stair and leaning against a window.\", \"A man in a blue shirt is standing on a ladder cleaning a window.\", \"A man on a ladder cleans the window of a tall building.\", \"Man in blue shirt and jeans on ladder cleaning windows\", \"A man on a ladder cleans a window\"]', '[15, 16, 17, 18, 19]', 'train', '1000344755.jpg', '3']\n"
     ]
    }
   ],
   "source": [
    "# let's open the csv file and check the first few lines to understand the format of the annotations\n",
    "import csv\n",
    "with open(\"/mnt/media/eleonora/flickr30k/flickr_annotations_30k.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < 5:  # Print the first 5 rows\n",
    "            print(row)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44ad7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Reproducibility (SEED)\n",
    "# ---------------------------\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # deterministic-ish (can slow down)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    # make dataloader workers deterministic\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Reproducibility (SEED)\n",
    "# ---------------------------\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # deterministic-ish (can slow down)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    # make dataloader workers deterministic\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c9da1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) Parse captions (5 per image)\n",
    "# ---------------------------\n",
    "def load_flickr30k_captions(captions_txt_path: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    captions.txt format (as in your screenshot):\n",
    "    first row header: image,caption\n",
    "    then: filename.jpg, caption text ...\n",
    "    \"\"\"\n",
    "    cap_dict = defaultdict(list)\n",
    "\n",
    "    with open(captions_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # skip header if present\n",
    "    start_idx = 1 if lines and lines[0].strip().lower().startswith(\"image,caption\") else 0\n",
    "\n",
    "    for line in lines[start_idx:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # split only on first comma because caption can contain commas\n",
    "        fname, caption = line.split(\",\", 1)\n",
    "        cap_dict[fname.strip()].append(caption.strip())\n",
    "\n",
    "    # sanity check: keep only images that have at least 1 caption\n",
    "    cap_dict = {k: v for k, v in cap_dict.items() if len(v) > 0}\n",
    "    return cap_dict\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Split by IMAGE (80/10/10)\n",
    "# ---------------------------\n",
    "def split_filenames(\n",
    "    filenames: List[str],\n",
    "    seed: int = 42,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    ") -> Tuple[List[str], List[str], List[str]]:\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    filenames = list(filenames)\n",
    "    rng.shuffle(filenames)\n",
    "\n",
    "    n = len(filenames)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "    # remainder goes to test (handles rounding)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_files = filenames[:n_train]\n",
    "    val_files = filenames[n_train:n_train + n_val]\n",
    "    test_files = filenames[n_train + n_val:]\n",
    "\n",
    "    assert len(train_files) == n_train\n",
    "    assert len(val_files) == n_val\n",
    "    assert len(test_files) == n_test\n",
    "\n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Dataset: (image, random caption) without repeats for same image\n",
    "#    Implementation trick:\n",
    "#    Expand each image into 5 distinct (image, caption_i) pairs.\n",
    "#    Shuffle pairs in DataLoader => caption \"chosen at random\"\n",
    "#    and cannot repeat for same image because each caption index appears once.\n",
    "# ---------------------------\n",
    "class Flickr30kNoRepeatCaptionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir: str,\n",
    "        captions_by_file: Dict[str, List[str]],\n",
    "        filenames: List[str],\n",
    "        seed: int = 42,\n",
    "        transform=None,\n",
    "        require_n_captions: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        - filenames: list of image filenames included in this split\n",
    "        - captions_by_file: dict filename -> list of captions\n",
    "        - require_n_captions: if 5, we enforce exactly 5 captions (skip images that don't have 5)\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.seed = seed\n",
    "\n",
    "        # filter to files that exist and have enough captions\n",
    "        kept = []\n",
    "        for fn in filenames:\n",
    "            if fn not in captions_by_file:\n",
    "                continue\n",
    "            if require_n_captions is not None and len(captions_by_file[fn]) < require_n_captions:\n",
    "                continue\n",
    "            img_path = os.path.join(images_dir, fn)\n",
    "            if os.path.isfile(img_path):\n",
    "                kept.append(fn)\n",
    "\n",
    "        self.filenames = kept\n",
    "        self.captions_by_file = captions_by_file\n",
    "\n",
    "        # Build expanded index list: each (filename, caption_idx) appears once\n",
    "        rng = random.Random(seed)\n",
    "        self.pairs: List[Tuple[str, int]] = []\n",
    "        for fn in self.filenames:\n",
    "            caps = captions_by_file[fn]\n",
    "            # choose 5 captions, but shuffle their order (random choice without repetition)\n",
    "            idxs = list(range(len(caps)))\n",
    "            rng.shuffle(idxs)\n",
    "            idxs = idxs[:require_n_captions] if require_n_captions is not None else idxs\n",
    "            for ci in idxs:\n",
    "                self.pairs.append((fn, ci))\n",
    "\n",
    "        # Note: global shuffle will be handled by DataLoader(shuffle=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fn, cap_idx = self.pairs[idx]\n",
    "        img_path = os.path.join(self.images_dir, fn)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = self.captions_by_file[fn][cap_idx]\n",
    "        return image, caption, fn, cap_idx  # fn/cap_idx are handy for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec59f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images (unique) in splits:\n",
      "  train: 25426 val: 3178 test: 3179\n",
      "Pairs (image,caption) in splits (x5 per image):\n",
      "  train: 127130 val: 15890 test: 15895\n",
      "Example batch filenames: ['75893484.jpg', '5661996549.jpg', '6188883048.jpg', '4152974865.jpg', '2914022011.jpg', '3368379417.jpg', '7570232540.jpg', '14887980.jpg', '4242032129.jpg', '56012054.jpg', '2694426634.jpg', '2512682478.jpg', '2360159351.jpg', '3741664817.jpg', '3631839768.jpg', '1083240835.jpg', '7735272386.jpg', '230269706.jpg', '584726817.jpg', '2561212119.jpg', '132966111.jpg', '2103568100.jpg', '4725183955.jpg', '165764051.jpg', '101362650.jpg', '6617071067.jpg', '486720042.jpg', '4717627685.jpg', '2350400382.jpg', '4299244891.jpg', '320779082.jpg', '7558058046.jpg']\n",
      "Example batch captions: ['A blond woman on a cellphone looking at an advertisement .', '\" A young man wearing protective eye wear , works on a tire .\"', 'A girl in a white top and black shorts is touching a volleyball with both of her arms together .', 'A man wearing a safety jacket is in the cockpit of an airplane .', 'A man in a red shirt stands against a wall while the shadow of another man can be seen in front of him .', 'A person is jumping at the top of a sand dune .', '\" A swimmer , with a blue cap and blue goggles , swims in calm water .\"', 'Two oriental women working at a table .', 'Soccer players from opposing teams are looking to the sidelines .', 'A woman wearing a brown shirt is collecting water in a large tub', 'A man and two children in life jackets in a boat on a lake .', 'A little boy in a blue shirt and blue jeans is pushing the swing that a little boy in a red shirt and blue shorts is sitting in .', 'A boy jumps on a stone footing next to a lake .', 'There are two girls dressed in exercise clothing obviously doing an exercise on a beach .', 'A person feeding a tiny baby animal with a bottle .', 'A woman is applying mascara .', '\" A woman in an American military uniform sits at a table and writes the words \"\" sad , \"\" \"\" depressed , \"\" and \"\" hatred \"\" on a large sheet of white paper .\"', 'A boy petting a white bull .', 'Two women walking on a gray road .', 'Black dog stands in river .', 'A charity walk happening with balloons .', 'The white dog is wading in the pond atop the mountain .', 'A teenage boy uses an electronic device on the street .', 'A small boy is running to catch the ball .', 'A boy jumps over three other students in a martial arts demonstration .', 'A guy is doing a skateboard trick over stairs .', 'A black and white spotted dog is circling a gray and white dog .', 'A dog begging to a man and a woman .', 'A black dog jumps in the air to catch a toy .', 'Man in t-shirt and jeans cuts up cooked chicken .', 'A bunch of businessmen are gathered for a meeting .', '\" A man resting in a blue hammock , boots and pack off , as he looks out over a flowing river .\"']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 5) Put everything together\n",
    "# ---------------------------\n",
    "SEED = 123\n",
    "seed_everything(SEED)\n",
    "\n",
    "flickr_root = \"/mnt/media/eleonora/flickr30k\"\n",
    "captions_txt = os.path.join(flickr_root, \"captions.txt\")\n",
    "images_dir = os.path.join(flickr_root, \"Images\")  # adjust if your folder name differs\n",
    "\n",
    "captions_by_file = load_flickr30k_captions(captions_txt)\n",
    "\n",
    "all_files = sorted(list(captions_by_file.keys()))\n",
    "train_files, val_files, test_files = split_filenames(all_files, seed=SEED)\n",
    "\n",
    "# Optional: torchvision transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_ds = Flickr30kNoRepeatCaptionDataset(\n",
    "    images_dir=images_dir,\n",
    "    captions_by_file=captions_by_file,\n",
    "    filenames=train_files,\n",
    "    seed=SEED,\n",
    "    transform=transform,\n",
    "    require_n_captions=5,\n",
    ")\n",
    "val_ds = Flickr30kNoRepeatCaptionDataset(\n",
    "    images_dir=images_dir,\n",
    "    captions_by_file=captions_by_file,\n",
    "    filenames=val_files,\n",
    "    seed=SEED + 1,   # different but deterministic\n",
    "    transform=transform,\n",
    "    require_n_captions=5,\n",
    ")\n",
    "test_ds = Flickr30kNoRepeatCaptionDataset(\n",
    "    images_dir=images_dir,\n",
    "    captions_by_file=captions_by_file,\n",
    "    filenames=test_files,\n",
    "    seed=SEED + 2,   # different but deterministic\n",
    "    transform=transform,\n",
    "    require_n_captions=5,\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,              # shuffle pairs => \"caption scelta a caso\"\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=seed_worker,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=seed_worker,\n",
    ")\n",
    "\n",
    "print(\"Images (unique) in splits:\")\n",
    "print(\"  train:\", len(train_files), \"val:\", len(val_files), \"test:\", len(test_files))\n",
    "print(\"Pairs (image,caption) in splits (x5 per image):\")\n",
    "print(\"  train:\", len(train_ds), \"val:\", len(val_ds), \"test:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ffd665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example batch filenames: ['4021561862.jpg', '4755772591.jpg', '2250479700.jpg', '4368364278.jpg', '6864033895.jpg', '4843297992.jpg', '4496738245.jpg', '5851818256.jpg', '155210731.jpg', '254901702.jpg', '189100641.jpg', '287967163.jpg', '2759879165.jpg', '480858814.jpg', '3446299757.jpg', '67110478.jpg', '3476381830.jpg', '1474240647.jpg', '2287938451.jpg', '4948224114.jpg', '4731305489.jpg', '4653315864.jpg', '3030953639.jpg', '3303787342.jpg', '3701226275.jpg', '3758747095.jpg', '4400684369.jpg', '316577571.jpg', '2909875716.jpg', '7725132206.jpg', '3439390288.jpg', '469386480.jpg']\n",
      "Example batch captions: ['The football player in the white jersey is running towards the football player in the light blue jersey .', 'A few people are staring at something .', 'Man and woman sitting with yarn and knitting needles .', 'A man and woman with name tags on chat with others in a hotel lobby .', 'A woman is juggling three oranges outside .', 'A group of bushy haired people are walking down a rainy sidewalk .', 'Four people on a balcony .', 'Three people riding horses in front of a small crowd .', '\" An Asian woman wearing a Asian dress sitting among a group of cloths , with a woven basket on her lap .\"', 'A man in a yellow sweater catching a Frisbee .', '\" A woman with brown hair , singing into a microphone .\"', '\" Two men , one of which is wearing a red shirt and a white , skirt-like thing over his black pants .\"', 'A stout gentleman does not know how to use his umbrella .', 'girl in cowboy hat looks unhappy', 'A older woman cleaning the window of a door .', 'A woman who is wearing a peach top and black jeans is bowling by herself .', 'A dancer on steps .', 'A man wearing a blue shirt and tan pants crossing the street .', 'An old man wearing a black hat is sleeping in a blue sleeping bag on grass .', 'A group of people dressed in red and blue stand in front of a door arch .', '\" A group of people walking through town on a cobblestone street , with a dark , ominous sky overhead .\"', 'A man holding a blond woman in a white tank top at night in a city .', '\" Several people are smiling as they wear crowns , garish hats , and painted faces .\"', 'A person bicycling through deep snow .', 'A boy is posing with a metal ladle in each hand .', '\" A very young boy is sitting in a swing in a park , smiling and posing with his arms above him grabbing the swing cables .\"', '\" A group of three men and two woman sitting at a table that has food , drinks , and a game on it .\"', 'A white dog and a black and white dog are running through the grass .', 'A young woman in a sunny meadow watches her yellow scarf blow in the wind .', 'Four women in swimsuits playing volleyball on a beach with cameramen recording in the background .', 'A blond girl in a purple flowered outfit and red shoes skips while on a subway car .', 'A woman and man playing guitar while a brown-haired woman speaks on the phone .']\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: same image won't reuse same caption index inside the dataset\n",
    "# (each (fn, cap_idx) appears once)\n",
    "batch = next(iter(train_loader))\n",
    "images, captions, fns, cap_idxs = batch\n",
    "print(\"Example batch filenames:\", list(fns))\n",
    "print(\"Example batch captions:\", list(captions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82241160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emanuele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
